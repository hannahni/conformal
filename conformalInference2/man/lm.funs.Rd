% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lm.R
\name{lm.funs}
\alias{lm.funs}
\title{Linear regression training and prediction functions, and special
  leave-one-out fitting function.}
\usage{
lm.funs(intercept = TRUE, lambda = 0)
}
\arguments{
\item{intercept}{Should an intercept be included in the linear model? Default
is TRUE.}

\item{lambda}{A value or sequence of lambda values to be used in ridge
regression. Default is 0, which means ordinary linear regression.}
}
\value{
A list with four components: train.fun, predict.fun, special.fun,
  active.fun. The last function is designed to take the output of train.fun,
  and reports which features are active for each fitted model contained in
  this output. Trivially, here, all features are generically active in ridged
  linear models.
}
\description{
Construct the training and prediction functions for linear regression, and 
  a special function to quickly compute leave-one-out fitted values.
}
\details{
The train.fun function constructed here leverages an optional third
  argument (in addition to the usual x,y): out, whose default is NULL. If
  non-NULL, it is assumed to be the result of a previous call to train.fun,
  on the \emph{same} features x as we are currently considering. This is done
  for efficiency, and to perform the regression, it uses the saved Cholesky
  decomposition instead of computing a new one from scratch.
}
\examples{
## Linear regression: classical setting

# Generate some example training data
set.seed(33)
n = 100; p = 10
x = matrix(rnorm(n*p),n,p)
beta = rnorm(p)
y = x \%*\% beta + rnorm(n)

# Generate some example test data
n0 = 50
x0 = matrix(rnorm(n0*p),n0,p)
y0 = x0 \%*\% beta + rnorm(n0)

# Linear regression training and prediction functions
funs = lm.funs()

# Conformal inference, and split conformal inference
out.conf = conformal.pred(x, y, x0, alpha=0.1,
  train.fun=funs$train, predict.fun=funs$predict, verb=TRUE)

out.split = conformal.pred.split(x, y, x0, alpha=0.1,
  train.fun=funs$train, predict.fun=funs$predict)

cov.conf = mean(out.conf$lo <= y0 & y0 <= out.conf$up)
len.conf = mean(out.conf$up - out.conf$lo)
err.conf = mean((y0 - out.conf$pred)^2)

cov.split = mean(out.split$lo <= y0 & y0 <= out.split$up)
len.split = mean(out.split$up - out.split$lo)
err.split = mean((y0 - out.split$pred)^2)

# Compare to parametric intervals from linear regression
lm.orac = lm(y~x[,1:p])
out.orac = predict(lm.orac, list(x=x0[,1:p]),
  interval="predict", level=0.9)

cov.orac = mean(out.orac[,"lwr"] <= y0 & y0 <= out.orac[,"upr"])
len.orac = mean(out.orac[,"upr"] - out.orac[,"lwr"])
err.orac = mean((y0 - out.orac[,"fit"])^2)

tab = matrix(c(cov.conf,len.conf,err.conf,
  cov.split,len.split,err.split,
  cov.orac,len.orac,err.orac),ncol=3)
colnames(tab) = c("Conformal","Split conformal","Oracle")
rownames(tab) = c("Avg coverage","Avg length","Test error")
tab

plot(y0, ylim=c(min(out.conf$lo,out.split$lo,out.orac[,"lwr"]),
           max(out.conf$up,out.split$up,out.orac[,"upr"])),
     main="Linear regression: prediction intervals")
segments(1:n0, out.conf$lo, 1:n0, out.conf$up)
segments(1:n0+0.2, out.split$lo, 1:n0+0.2, out.split$up, col=4)
segments(1:n0-0.2, out.orac[,"lwr"], 1:n0-0.2, out.orac[,"upr"], lty=2, col=2)
legend("topleft",col=c(1,4,2),lty=c(1,1,2),
       legend=c("Conformal","Split conformal","Oracle"))
}
\author{
Ryan Tibshirani
}
